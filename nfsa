#!/usr/bin/python

def get_annotations(opt,id3):
    from dataset import Variable
    import json
    from itertools import product
    from util import scalar,ip2int,int2ip

    flow = Variable('flow')
    src = Variable('src')
    dst = Variable('dst')
    dport = Variable('dport')
    sport = Variable('sport')

    # some heuristics
    if not opt.annotations:
        raise Exception('no annotation file specified')
    f = open(opt.annotations)
    filters = json.load(f)
    labeling3 = {}
    labeling2 = {}
    labeling = {}
    i = 1
    for f in filters:
        for k in ('dstIPs','dstPorts','srcIPs','dstPorts'):
            if k not in f:
                f[k] = []
        if 'fileName' in f:
            annot = '%s (%s)' %(f['fileName'],f['annotation'])
        else:
            annot = f['annotation']
        if f['type'] not in labeling2:
            labeling2[annot] = i
            labeling3[i] = f['type']
            i += 1
        a = labeling2[annot]
        if f['dstIPs'] and f['dstPorts'] and f['srcIPs']:
            for d,dp,s in product(f['dstIPs'],f['dstPorts'],f['srcIPs']):
                for r in id3.select((src==ip2int(s))&(dst==ip2int(d))&(dport==dp),fields=('flow',)):
                    labeling[scalar(r)] = a
        elif f['dstIPs'] and f['dstPorts']:
            for d,dp in product(f['dstIPs'],f['dstPorts']):
                for r in id3.select((dst==ip2int(d))&(dport==dp),fields=('flow',)):
                    labeling[scalar(r)] = a
        elif f['dstIPs']:
            for d in f['dstIPs']:
                for r in id3.select((dst==ip2int(d)),fields=('flow',)):
                    labeling[scalar(r)] = a
        elif f['srcIPs']:
            for d in f['srcIPs']:
                for r in id3.select((src==ip2int(d)),fields=('flow',)):
                    labeling[scalar(r)] = a
    return labeling,dict( (v,k) for k,v in labeling2.items() ), labeling3

def get_labeling(opt,id3):
    from dataset import Variable
    import json
    from itertools import product
    from util import scalar,ip2int,int2ip

    flow = Variable('flow')
    src = Variable('src')
    dst = Variable('dst')
    dport = Variable('dport')
    sport = Variable('sport')

    # some heuristics
    if not opt.annotations:
        raise Exception('no annotation file specified')
    f = open(opt.annotations)
    filters = json.load(f)
    labeling2 = {}
    labeling = {}
    for f in filters:
        for k in ('dstIPs','dstPorts','srcIPs','dstPorts'):
            if k not in f:
                f[k] = []
        if f['type'] not in labeling2:
            if f['type'] == 'FILTER_LEGITIMATE':
                labeling2[f['type']] = 1
            elif f['type'] == 'FILTER_MALICIOUS':
                labeling2[f['type']] = 2
            else: labeling2[f['type']] = None
        a = labeling2[f['type']]
        if f['dstIPs'] and f['dstPorts'] and f['srcIPs']:
            for d,dp,s in product(f['dstIPs'],f['dstPorts'],f['srcIPs']):
                for r in id3.select((src==ip2int(s))&(dst==ip2int(d))&(dport==dp),fields=('flow',)):
                    labeling[scalar(r)] = a
        elif f['dstIPs'] and f['dstPorts']:
            for d,dp in product(f['dstIPs'],f['dstPorts']):
                for r in id3.select((dst==ip2int(d))&(dport==dp),fields=('flow',)):
                    labeling[scalar(r)] = a
        elif f['dstIPs']:
            for d in f['dstIPs']:
                for r in id3.select((dst==ip2int(d)),fields=('flow',)):
                    labeling[scalar(r)] = a
        elif f['srcIPs']:
            for d in f['srcIPs']:
                for r in id3.select((src==ip2int(d)),fields=('flow',)):
                    labeling[scalar(r)] = a
    return labeling,dict( (v,k) for k,v in labeling2.items() )
def show_spectra(opt,h5=None):
    from models import pipeline,freq_bands,freq_treshold
    if not h5:
        h5 = File(opt.database,'a')
    if 'samples' not in h5:
        raise Exception('no samples found')
    samples = h5['samples']
    for k in samples.keys():
        if k.startswith('data'):
            if  '.srate' not in samples[k] or '.wndsize' not in samples[k]:
                continue
            if samples[k]['.srate'].value !=  opt.srate  or  samples[k]['.wndsize'].value != opt.window:
                continue
            X = samples[k]['X'][:]
            y = samples[k]['y'][:].squeeze()
            freqs = samples[k]['freqs'][:].squeeze()
            id = Dataset(h5=samples[k]['flowids'])
            labeling,labeling2,labeling3 =  get_annotations(opt,id)
            lab  = np.array([labeling.get(f) for f in y.squeeze()])
            fnc = pipeline(freq_treshold(0), freq_bands(50))
            for i in  labeling2:
                if not (lab==i).any():
                    continue
                Xr,yr,freqr = fnc.fit(X[(lab==i) ,...],lab[(lab==i)],freqs)
                freqr = freqr.squeeze()
                fig(lambda ax: ax.bar(left=freqr[:-1],width=freqr[1:]-freqr[:-1],height=Xr.mean(0)),name=labeling2[i])
def show_mahal(opt,h5=None):
    from models import pipeline,freq_bands,freq_treshold
    if not h5:
        h5 = File(opt.database,'a')
    if 'samples' not in h5:
        raise Exception('no samples found')
    samples = h5['samples']
    for k in samples.keys():
        if k.startswith('data'):
            if  '.srate' not in samples[k] or '.wndsize' not in samples[k]:
                continue
            if samples[k]['.srate'].value !=  opt.srate  or  samples[k]['.wndsize'].value != opt.window:
                continue
            X = samples[k]['X'][:]
            y = samples[k]['y'][:].squeeze()
            freqs = samples[k]['freqs'][:].squeeze()
            id = Dataset(h5=samples[k]['flowids'])
            labeling,labeling2,labeling3 =  get_annotations(opt,id)
            lab  = np.array([labeling.get(f) for f in y.squeeze()])
            fnc = pipeline(freq_treshold(0), freq_bands(50))
            for i in  labeling2:
                if not (lab==i).any():
                    continue
                Xr,yr,freqr = fnc.fit(X[(lab==i) ,...],lab[(lab==i)],freqs)
                freqr = freqr.squeeze()
                fig(lambda ax: ax.bar(left=freqr[:-1],width=freqr[1:]-freqr[:-1],height=Xr.mean(0)),name=labeling2[i])
def get_raw(opt, callback=None, keys=(), h5=None):
    from extractor import PcapExtractor,FlowExtractor
    from os.path import isfile,basename
    from util import get_packets

    if opt.in_format == 'pcap':
        extract = PcapExtractor( ('time','src','sport','dst','dport','proto','paylen','flags', 'flow') )
        tr = h5.require_group('traces') if h5 else None
        praser = get_packets
    elif opt.in_format == 'netflow':
        extract = FlowExtractor( ('time', 'duration','src','sport','dst','dport','proto', 'packets', 'size','flags', 'flows', 'flow') )
        tr = h5.require_group('netflows')  if h5 else None
        def praser(fn, extract):
            f = open(fn,'r')
            try: flows = f.readlines()
            finally: f.close()
            return filter(None,map(extract,flows))
    else:
        raise NotImplementedError('in_format')

    if tr:
        if not callable(callback):
            callback = lambda data,fn: data.save(tr.require_group(fn))
        if not keys:
            keys = tr.keys()
    else:
        if not callable(callback):
            raise Exception('h5 file needed')

    for fn in opt.file:
        if isfile(fn) and basename(fn) not in keys:
            print '## Extracting features from file %s...' % fn
            pkts = praser(fn,extract)
            print '\t%d records captured'%len(pkts)
            data = Dataset(pkts, extract.fields)
            del pkts
            print '## Storing matrix in %s...' % opt.database
            callback(data,basename(fn))

def get_flow(opt, h5 = None):
    from util import timedrun
    from flowizer import Flowizer
    from operator import add

    result = [[],()]
    def process(data,flowize,postfix=''):
        print '## Extracting flows using %s-tuple'%opt.flowid
        fl = h5.require_group(flowid)
        q,f = flowize(data, Dataset(h5=fl['flowid'])) if 'flowid' in fl else flowize(data)
        dataname= 'data_%s'%postfix if postfix else 'data'
        for i in (dataname,'flowid'):
            if i in fl:
                del fl[i]
        print '## Storing matrices in %s...' % opt.database
        f.save(fl.require_group(dataname))
        q.save(fl.require_group('flowid'))
        result[0].append(f)
        result[1] = q

    if not h5 :
        h5 = File(opt.database,'a')

    if opt.flowid == '3':
        fflow=('src','dst','dport')
        bflow=('dst','src','sport')
        flowid = 'flows3'
    elif opt.flowid == '4':
        fflow=('src', 'sport','dst','dport')
        bflow=('dst', 'dport','src','sport')
        flowid = 'flows4'
    else:
        raise NotImplementedError('flowid')

    if opt.in_format == 'pcap':
        fields = ('time', 'paylen', 'flow')
        tr = h5['traces'] if 'traces' in h5 else None
    elif opt.in_format == 'netflow':
        fields = ('time', 'size', 'packets', 'flow')
        tr = h5['netflows'] if 'netflows' in h5 else None
    else:
        raise NotImplementedError('in_format')

    flowize = timedrun(Flowizer(fields = fields,fflow=fflow,bflow=bflow,opt=opt))

    if not tr:
        fl = h5.require_group(flowid)
        keys = [ i[1] for i in (k.split('_',1) for k in fl.keys()) if len(i) > 1 ]
        get_raw(opt,lambda x,fn:process(x,flowize,fn),keys=keys)
    else:
        try:
            data = reduce(add,(Dataset(h5=tr[k]) for k in sorted(tr.keys()))  )
            process(data,flowize)
        except MemoryError:
            for k in sorted(tr.keys()):
                data = Dataset(h5=tr[k])
                process(data,flowize,k)
    return tuple(result)

def get_samples(opt, h5= None):
    from sampler import csd,psd
    from dataset import Variable,Dataset
    from scipy.fftpack import fftfreq
    from sys import stdout
    from itertools import product
    from h5py import File
    from operator import add
    from util import flow2str,scalar

    if opt.flowid == '3':
        flowid = 'flows3'
    elif opt.flowid == '4':
        flowid = 'flows4'
    else:
        raise NotImplementedError('flowid')

    if opt.transform == 'psd':
        xsdfnc = psd
    elif opt.transform == 'csd':
        xsdfnc = csd
    else:
        raise NotImplementedError('transform')

    if opt.srate:
        srates = opt.srate
    else:
        raise ValueError('No sample rate specified.')

    if opt.window:
        windows = opt.window
    else:
        raise ValueError('No window length specified.')

    if not h5:
        h5 = File(opt.database,'a')

    if flowid in h5:
        fl3 = h5[flowid]
        if 'data' in fl3:
            flows3 = Dataset(h5=fl3['data'])
        else:
            keys = [k for k in fl3.keys() if k.startswith('data_')]
            if keys:
                flows3 = reduce(add,(Dataset(h5=fl3[k]) for k in keys) )
            else:
                print '## no flow data in h5 database, trying to get it from raw files'
                if not opt.file:
                    raise Exception('No flow data specified. Try to specify some pcap or netflow files on command line.')
                flows3,id3 = get_flow(opt,h5 = h5)
                flows3 = reduce(add, flows3 )
        id3 = Dataset(h5=fl3['flowid'])
    else:
        flows3,id3 = get_flow(opt,h5 = h5)
        flows3 = reduce(add, flows3 )

    samples = h5.require_group('samples')

    for srate,wndsize in product(srates,windows) : #srates:
        if srate<=0 :
            continue


        speriod = 1./ srate # sampling period in seconds
        wndspan = int(1e6 * wndsize * speriod) # window span in microseconds

        flow =  Variable('flow')
        time =  Variable('time')

        sampl = samples.require_group('data_%s_%f_%d'%(xsdfnc.__name__,srate,wndsize))

        if ( '.srate' in sampl or '.wndsize' in sampl ) and ( sampl['.srate'] != srate or sampl['.wndsize'] != wndsize ):
            raise Exception('already processed for different srate and wndsize')

        spectrums = {}
        amplitudes = {}
        wids = {}
        ips = {}

        # some colorful sugar
        ipfmt = lambda i: flow2str(i,dns=opt.reverse_dns,services=opt.reverse_dns, color=False)
        ipfmtc = lambda i: flow2str(i,dns=opt.reverse_dns,services=opt.reverse_dns, color=True)

        stdout.write('\n')
        stdout.flush()

        flows = {}
        for f in flows3['flow']:
            f = scalar(f)
            if f not in flows:
                flows[f] = 0
            else:
                flows[f] += 1
        flows = dict((k,v) for k,v in flows.items() if v>100)

        l = 1
        #for f in flid[:chooseflows,0]:
        for id3row in id3:
            f = scalar(id3row['flow'])
            if f not in flows:
                continue

            ips[f] = ipfmt(id3row)

            stdout.write('\rprogress: \033[33;1m%0.2f %%\033[0m, srate= \033[33;1m%f\033[0m Hz, wndsize= \033[33;1m%d\033[0m, \033[36mprocessing flow\033[0m: %s  '%
                         (100.*l/len(flows),srate,wndsize,ipfmtc(id3row)))
            stdout.flush()
            l += 1

            # select related packets
            #fl =  flows[(flows[...,2] == f ),...]
            fl =  flows3.select(flow==f,retdset=True)
            tm = fl['time']
            mi = tm.min()
            ma = tm.max()

            k = mi
            i = 0
            spectrum = []
            amplitude = []
            wid = []
            unused = 0

            while k<ma:
                # 10 dots progressbar
                if (ma-mi)>=(10*wndspan) and  not ((k-mi)/wndspan) % (((ma-mi)/(10*wndspan))):
                    stdout.write('\033[36m.\033[0m')
                    stdout.flush()

                #w = fl.data[(tm>=k) & (tm<k+wndsize*srate),...]
                if 'paylen' in fl:
                    w = fl.select((time>=k)&(time<k+wndspan),retdset=True,fields=('time','paylen'))
                else:
                    w = fl.select((time>=k)&(time<k+wndspan),retdset=True,fields=('time','packets','size'))

                if not len(w)>0:
                    unused += np.sum(w['packets']) if 'packets' in w else len(w)
                    k += wndspan
                    i += 1
                    continue

                # sampling intervals
                bounds = np.linspace(k, k+wndspan, wndsize, endpoint=True)[...,np.newaxis]

                amp,xsd = xsdfnc(w,bounds)

                if not xsd.any():
                    unused += np.sum(w['packets']) if 'packets' in w else len(w)
                    k += wndspan
                    i += 1
                    continue

                wid.append(i)
                spectrum.append(xsd)
                amplitude.append(amp)

                k += wndspan
                i += 1

            pkts = np.sum(fl['packets']) if 'packets' in fl else len(fl)

            if  len(amplitude):
                amplitude = np.vstack(a[np.newaxis,...] for a in amplitude)
                spectrum = np.vstack(a[np.newaxis,...] for a in spectrum)
                #amplitudes[f] = amplitude
                spectrums[f] = spectrum
                wids[f] = np.array(wid)
                if unused:
                    stdout.write('\r%s: unused \033[1;31m%d\033[0m of \033[1;34m%d\033[0m packets\033[K\n' %(ipfmtc(id3row),unused,pkts))
                else:
                    stdout.write('\r%s: used \033[1;34m%d\033[0m packets\033[K\n' %(ipfmtc(id3row),pkts))
            else:
                stdout.write('\r%s: unused \033[1;31m%d\033[0m packets\033[K\n' %(ipfmtc(id3row),pkts))
            stdout.write('\rprogress: \033[33;1m%0.2f %%\033[0m, srate= \033[33;1m%f\033[0m Hz, wndsize= \033[33;1m%d\033[0m   '%(100.*l/len(flows),srate,wndsize))
            stdout.flush()

        stdout.write('\rprogress: \033[33;1m100 %%\033[0m, srate= \033[33;1m%f\033[0m Hz, wndsize= \033[33;1m%d\033[0m           \n'% (srate,wndsize))
        stdout.flush()

        if len(spectrums):
            flows = list(spectrums.keys())

            X = np.vstack(spectrums[f] for f in flows) # spectrums
            #ampl = np.vstack(amplitudes[f] for f in flows) # amplitudes
            y = np.vstack(np.array([[f]]).repeat(spectrums[f].shape[0],0) for f in flows) # flows
            #wnds = np.vstack(wids[f][...,np.newaxis] for f in flows) # windows kept

            sampl.create_dataset('.srate',data=srate)
            sampl.create_dataset('.wndsize',data=wndsize)

            sampl.create_dataset('X',data=X)
            sampl.create_dataset('y',data=y)
            #sampl.create_dataset('A',data=ampl)
            #sampl.create_dataset('wnds',data=wnds)
            id3.save(sampl.require_group('flowids'))
            sampl.create_dataset('freqs', data = fftfreq(wndsize-1,d=1./srate))


def annotate(opt, h5=None):
    from util import flow2str,scalar
    from operator import add
    from sys import stdout
    import json

    def feedback(i):
        types = {'2':'FILTER_MALICIOUS','1':'FILTER_LEGITIMATE', '0': None}
        input = ()
        while len(input)!=2 or input[0].strip() not in types:
            input = raw_input('%s ? '%flow2str(i,dns=True,services=True)).split(',',1)
        return {'type':types.get(input[0].strip()),'annotation':input[1],'data':i}

    if not h5 :
        h5 = File(opt.database,'a')

    if opt.flowid == '3':
        flowid = 'flows3'
    elif opt.flowid == '4':
        flowid = 'flows4'
    else:
        raise NotImplementedError('flowid')

    d  =Dataset(h5= h5['%s/flowid'%flowid] )
    data = reduce(add,  (Dataset(h5=h5['%s/%s'%(flowid,k)]) for k in h5['flows3'].keys() if k.startswith('data')))
    flows = [i for i in reversed(sorted(((i,len(data[data.flow==i['flow']])) for i in d),key=lambda x: x[-1]))]
    min_packets =  opt.min_packets if opt.min_packets else 100
    flows = [i for i,c in flows if c>min_packets]
    print '## %d records to annotate'%len(flows)
    filt = [feedback(i) for i in sorted(flows,key=lambda x: x['dport'])]
    filt2 = [
        {
            'dstIPs':list(set( int2ip(scalar(j['data']['dst'])) for j in filt if j['annotation'] == a)),
            'dstPorts':list(set( scalar(j['data']['dport']) for j in filt if j['annotation'] == a)),
            'annotation':a,
            'type':t
        } for a,t in set((i['annotation'],i['type']) for i in filt)
    ]
    try:
        out = open(opt.out_file,'w') if  opt.out_file else stdout
    except :
        out =  stdout
    json.dump(filt2,out)

def opts(args=None):
    from argparse import ArgumentParser
    from sys import argv
    from info import __description__,__version__,__prog__,__date__,__author__

    if not args: args = argv[1:]

    actions = ('raw','flow','sample','model','filter', 'load', 'annotate')
    flowids  = ('3','4')
    transforms  = ('csd','psd')
    filetypes  = ('pcap','netflow')

    parser = ArgumentParser(description=__description__)


    parser.add_argument('--version', action='version', version='%s v%s\nCopyright (C) %s %s'%(__prog__,__version__,__date__.split('-',1)[0],__author__), help= 'show version information')

    parser.add_argument('action',choices=actions, metavar='(%s)'%('|'.join(actions)), help= ''\
            'action to execute; raw stores "pcap" or netflow data in h5 database, "flow" marks flows and extracts attributes, '\
            '"sample" computes sampling at given sample rate and tranformations at given windowing, "model" fits '\
            'model to data stored in database, filter converts XML Ip filters to JSON format and "load" loads'\
            ' database into memory')
    parser.add_argument('database', metavar='<database file>', help='hdf5 array database')
    parser.add_argument('file', nargs='*', metavar='<input file>', help='input files to process')

    parser.add_argument('-f', dest='in_format',choices=filetypes, metavar='(%s)'%('|'.join(filetypes)), help='input file format')
    parser.add_argument('-o', dest='out_file', metavar='<output file>', help='output file')
    parser.add_argument('-m', dest='min_packets', metavar='<min packets>', type=int, help='min packets per flow')
    parser.add_argument('-n', dest='reverse_dns', action='store_false', help='don`t do reverse dns')

    group = parser.add_mutually_exclusive_group()
    group.add_argument('-v', '--verbose', dest='verbosity', action='count', help='increase verbosity')
    group.add_argument('-q', '--quiet', dest='verbose', action='store_false', help='do not dump to terminal')

    group= parser.add_argument_group('Flow extraction options', 'Required for "flow", "sample" and "model" actions')
    group.add_argument('-i', dest='flowid',choices=flowids, metavar='(%s)'%('|'.join(flowids)), help='flow identification (3-tuple or 4-tuple)')
    group.add_argument('-u', dest='usesyns', action='store_true', help='don`t use SYN packets to distinguish flow start')
    group.add_argument('-p', dest='protocol', type=int, help='protocol to take in account, default = 6 (TCP)')

    group = parser.add_argument_group('Sampling options', 'Required for "sample" and "model" actions')
    group.add_argument('-s', dest='srate', metavar='<sample rate>',action='append',type=float,help='sample rate to use, can be specified multiple times')
    group.add_argument('-w', dest='window', metavar='<window length>',action='append',type=int,help='window lengths to use, can be specified multiple times')
    group.add_argument('-t', dest='transform', metavar='(%s)'%('|'.join(transforms)), choices=transforms,help=''\
          'tranformation to use, can be: "csd" for cross spectral density or "psd" for power spectral density')

    group = parser.add_argument_group('Model estimation options', 'Required for "model" action')
    group.add_argument('-a', dest='annotations', metavar='<file>', help='annotation file')
    #group.add_argument()

    parser.set_defaults(verbose=True,verbosity=0,reverse_dns=True,usesyns=False,protocol=6)

    #parser.print_help()
    longopts =  dict((v.dest,k) for k,v in  parser._option_string_actions.iteritems() if k.startswith('--'))
    shortopts =  dict((v.dest,k) for k,v in  parser._option_string_actions.iteritems() if not k.startswith('--'))
    opt =  parser.parse_args(args)

    if opt.action in ('raw','flow') and not opt.in_format:
        parser.error('input file format (--input-format) not specified for "raw" or "flow" action')

    if opt.action in ('flow', 'sample') and not opt.flowid:
        parser.error('flow identification (--flowid) not specified for "flow" or "sample" action')

    if opt.action in ('sample') and not opt.srate:
        parser.error('sample rate (--srate) not specified for "sample" action')

    if opt.action in ('sample') and not opt.window:
        parser.error('window (--window) not specified for "sample" action')

    if opt.action in ('sample') and not opt.transform:
        parser.error('transform (--transform) not specified for "sample" action')

    return opt, longopts, shortopts



if __name__=='__main__':
    from util import ip2int,int2ip,reverseDns,fig
    import numpy as np
    from dataset import Dataset
    from h5py import File
    import util
    import models

    opt, longopts, shortopts = opts()
    NotImplementedError = lambda s: ValueError('Choice %s %s not implemented'%(longopts.get(s) or shortopts.get(s),getattr(opt,s)))


    if opt.action == 'filters':

        import json
        from os.path import isfile
        from util import get_filter
        from sys import stdout
        f = open(opt.out_file,'w') if opt.out_file else stdout
        try:
            print json.dump([get_filter(f.strip()) for f in opt.file if isfile(f)], f)
        finally:
            f.close()

    elif opt.action == 'load':

        h5=File(opt.database,'a')

    elif opt.action == 'raw':

        get_raw(opt,h5=File(opt.database,'a'))
        exit()

    elif opt.action == 'annotate':

        annotate(opt,h5=File(opt.database,'a'))
        exit()

    elif opt.action == 'flow':

        get_flow(opt, h5 = File(opt.database,'a'))
        exit()

    elif opt.action == 'sample':

        get_samples(opt, h5 = File(opt.database,'a'))
        exit()

    elif opt.action == 'model':
        from models import freq_treshold,freq_bands,freq_momentum,freq_sdev,freq_low_hi,pca,scale,gmm
        h5 = File(opt.database,'a')
        def get_sampl(i):
            from dataset import Variable
            sampl = samples[i]

            srate = sampl['.srate'].value
            wndsize = sampl['.wndsize'].value

            X = sampl['X'].value
            y = sampl['y'].value
            freqs = sampl['freqs'].value

            id3 = Dataset(h5=sampl['flowids'])

            labeling,labeling2 = get_labeling(opt,id3)

            labels = np.array([[labeling.get(f) for f in y.squeeze()]]) # annotations

            return X,np.array([[labeling.get(f) for f in y.squeeze()]]),freqs,srate,wndsize


        def iterate(name, *args):
            from itertools import product
            cmds = [arg[0] for arg  in args]
            params = product(*[arg[1] for arg  in args])
            return [ (tuple((cmds[i].__name__,param[i]) for i in range(len(cmds))),pipeline(*[cmds[i](param[i]) for i in range(len(cmds))])) for param in params ]


        def crossval(method, X, y, freqs, folds = 10):
            from sklearn.cross_validation import StratifiedKFold
            s = []
            d = []
            for train, test in StratifiedKFold(y.squeeze(), folds, indices=False):
                try:
                    method.fit(X[train,...],y[...,train],freqs)
                    (score,data),dummy,dummy = method.score(X[test,...], y[...,test], freqs)
                    s += score,
                    d += data,
                except FitError:
                    s+=-np.inf,
            return np.array(s),d
        try:
            methods = {}
            methods.update(iterate( 'bands', ( freq_treshold, (0,)) , ( freq_bands, (2,5,10)) , ( scale, (True,)) ,  ( gmm, (1,) ) ))
            #methods.update(iterate( 'sdev', ( freq_treshold, (0,)) , ( freq_sdev, (0,)) , ( scale, (True,)) ,  ( gmm, (1,) ) ))
            methods.update(iterate( 'momentum', ( freq_treshold, (0,)) , ( freq_momentum, (2,3,4)) , ( scale, (True,)) ,  ( gmm, (1,) ) ))
            #methods.update(iterate( 'lowhi', ( freq_treshold, (0,)) ,( freq_low_hi, (5,10,20,40)) , ( scale, (True,)) ,  ( gmm, (1,) ) ))
            methods.update(iterate( 'pca', ( freq_treshold, (0,)) ,  ( pca, (3,5,7)) , ( scale, (True,)) ,  ( gmm, (1,) ) ))
            #methods.update(iterate( 'lowhi', ( freq_treshold, (0,)) ,( freq_low_hi, (5e-6,6e-5,1.1e-4,1.6e-4,2.2e-4, 2.7e-4, 3.2e-4, 3.8e-4, 4.4e-4, 5e-5 )) , ( scale, (True,)) ,  ( gmm, (1,) ) ))
            samples = h5['samples']
            for k in samples.keys():
                try:
                    if not k.startswith('data'):
                        continue
                    X, y, freqs, srate, wndsize = get_sampl(k)
                    if srate not in opt.srate or wndsize not in opt.window:
                        continue
                    print 'srate=%fHz, wndsize=%dsamps'%(srate,wndsize)
                    f = []
                    n = []
                    scores = []
                    for p,meth in methods.items():
                        s,d = crossval(meth, X, y, freqs)
                        name = '\t%s: score (mean=%f, std=%f)' % (','.join('%s(%d)'%n for n in p), s.mean(), s.std())
                        f += d[0],
                        n += 'srate=%f, %s' % (srate,'%s(%d)' % p[1]),
                        scores += s,
                        print name
                    def plotline(x,y):
                        return lambda ax: ax.plot(x, y, '-')
                    fig(list(plotline(fpr, tpr) for fpr, tpr, t in f),name=n,show=True)
                except Exception as e:
                    print e
                    #raise
        finally:
            h5.close()
    elif opt.action == 'feature':
        from dataset import Variable,Dataset
        from scipy.fftpack import fftfreq,fft
        from sys import stdout
        from util import scatter,scalar

        flow = Variable('flow')
        src = Variable('src')
        dst = Variable('dst')
        dport = Variable('dport')
        sport = Variable('sport')

        h5 = File(opt.database,'a')

        if len(argv)>3:
        #for grp in (k for k in h5.keys() if k.startswith('samples_')):
        #for i in (100,200,500,1000,2000):
            grp = argv[3]
            #if grp not in h5:
            #    exit() # continue

            sampl = h5[grp]

            srate = sampl['.srate'].value
            wndsize = sampl['.wndsize'].value

            X = sampl['X'].value
            y = sampl['y'].value
            freqs = sampl['freqs'].value

            id3 = Dataset(data=sampl['id'].value,fields=sampl['idfields'].value)
            labeling,labeling2 = get_labeling(id3)
            labels = np.array([[labeling.get(f) for f in y.squeeze()]]) # annotations

            def pcatransform(ndim=2):
                from sklearn.decomposition import PCA
                t = PCA(ndim)
                def fnc(X):
                    t.fit(X[labels.squeeze()==1,...])
                    return t.transform(X)
                return fnc
            def lowhitransform(srate, wndsize, fthresh=50):
                from scipy.fftpack import fftfreq
                freqs = fftfreq(wndsize-1,d=1./srate)
                return lambda X: np.vstack((X[...,freqs<=fthresh].sum(1),X[...,freqs>fthresh].sum(1))).transpose()
            def varnorm(X):
                return (X - X.mean(0).reshape((1,X.shape[1])) )/X.std(0).reshape((1,X.shape[1]))
            def logistnorm(X):
                from scipy.special import expit
                return expit(varnorm(X))
            def scatter1(ax):
                return scatter(ax, X, labels, varnorm, pcatransform(ndim=2), labeling2.get)
            def scatter1a(ax):
                return scatter(ax, X, labels, varnorm, pcatransform(ndim=2), labeling2.get)
            def scatter1b(ax):
                return scatter(ax, X, labels, logistnorm, pcatransform(ndim=2), labeling2.get)
            def scatter2a(ax):
                return scatter(ax, X, labels, None, lowhitransform(srate,wndsize,fthresh=10), labeling2.get)
            def scatter2b(ax):
                return scatter(ax, X, labels, varnorm, lowhitransform(srate,wndsize,fthresh=10), labeling2.get)
                #fignames =  'spectral features (srate=%dHz, wnd=%ds), %%s, %%s' %(srate,wndsize/srate)
            #fignames = [fignames%('variance normalization','2D pca projection'),fignames%('logistic normalization','2D pca projection')]
            #fig([scatter1a,scatter1b], fignames,show=True)
            #fignames =  'spectral features (srate=%dHz, wnd=%fs), %%s, %%s' %(srate,1.*wndsize/srate)
            #fignames = [fignames%('no normalization','low-pass/hi-pass energy'),fignames%('variance normalization','low-pass/hi-pass energy')]
            fig(scatter1,show=True)
