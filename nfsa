#!/usr/bin/python

def get_annotations(opt,id3):
    #TODO -> model.py
    from dataset import Variable
    import json
    from itertools import product
    from util import scalar,ip2int,int2ip

    flow = Variable('flow')
    src = Variable('src')
    dst = Variable('dst')
    dport = Variable('dport')
    sport = Variable('sport')

    # some heuristics
    if not opt.annotations:
        raise Exception('no annotation file specified')
    f = open(opt.annotations)
    filters = json.load(f)
    labeling3 = {}
    labeling2 = {}
    labeling = {}
    i = 1
    for f in filters:
        for k in ('dstIPs','dstPorts','srcIPs','dstPorts'):
            if k not in f:
                f[k] = []
        if 'fileName' in f:
            annot = '%s (%s)' %(f['fileName'],f['annotation'])
        else:
            annot = f['annotation']
        if f['type'] not in labeling2:
            labeling2[annot] = i
            labeling3[i] = f['type']
            i += 1
        a = labeling2[annot]
        if f['dstIPs'] and f['dstPorts'] and f['srcIPs']:
            for d,dp,s in product(f['dstIPs'],f['dstPorts'],f['srcIPs']):
                for r in id3.select((src==ip2int(s))&(dst==ip2int(d))&(dport==dp),fields=('flow',)):
                    labeling[scalar(r)] = a
        elif f['dstIPs'] and f['dstPorts']:
            for d,dp in product(f['dstIPs'],f['dstPorts']):
                for r in id3.select((dst==ip2int(d))&(dport==dp),fields=('flow',)):
                    labeling[scalar(r)] = a
        elif f['dstIPs']:
            for d in f['dstIPs']:
                for r in id3.select((dst==ip2int(d)),fields=('flow',)):
                    labeling[scalar(r)] = a
        elif f['srcIPs']:
            for d in f['srcIPs']:
                for r in id3.select((src==ip2int(d)),fields=('flow',)):
                    labeling[scalar(r)] = a
    return labeling,dict( (v,k) for k,v in labeling2.items() ), labeling3

def get_labeling(opt,id3):
    #TODO -> model.py
    from dataset import Variable
    import json
    from itertools import product
    from util import scalar,ip2int,int2ip

    if not opt.annotations:
        raise Exception('no annotation file specified')
    f = open(opt.annotations)
    filters = json.load(f)

    print '\033[34;1mannotations detected in filter file:\033[0m'
    i = 0
    annotations = {}
    for f in filters:
        annot = '%s (%s)' %(f['fileName'],f['annotation'])  if 'fileName' in f else f['annotation']
        annotations[i] = (f['type'],annot,f)
        if f['type'] == 'FILTER_LEGITIMATE':
            print '[\033[33;1m%d\033[0m] \033[32m%s\033[0m : %s'%(i,annotations[i][0],annotations[i][1])
        elif f['type'] == 'FILTER_MALICIOUS':
            print '[\033[33;1m%d\033[0m] \033[1;31m%s\033[0m : %s'%(i,annotations[i][0],annotations[i][1])
        else:
            print '[\033[33;1m%d\033[0m] \033[33m%s\033[0m : %s'%(i,annotations[i][0],annotations[i][1])
        i+=1
    def get_classes(msg):
        model = []
        while not model:
            s = raw_input(msg)
            model = [k for k,v in annotations.iteritems() if s in v[:-1]]
            if not model:
                try:
                    model = [ j for j in ( int(i.strip()) for i in  s.split(',') ) if j in annotations]
                except :
                    pass
        print model
        return model
    #'\033[32m%s\033[0m:\033[33m%s\033[0m > \033[32m%s\033[0m:\033[33m%s\033[0m'
    model  = get_classes('classes included in model: ')
    legit  = get_classes('normal classes: ')
    malicious  = get_classes('anomal classes: ')
    model_legit = None
    while model_legit is None:
        try: model_legit = bool(int(raw_input('model is normal: ')))
        except : pass
    print model_legit

    classes = {}
    classes.update((i,1) for i in legit if i not in model)
    classes.update((i,2) for i in malicious if i not in model)
    classes.update((i,-1 if model_legit else -2) for i in model)

    labeling = {
        1:'LEGIT',
        2:'MALICIOUS',
        -1:'LEGIT',
        -2:'MALICIOUS',
        0: 'UNKNOWN'
    }
    y = {}

    for i,(type,annot,f) in annotations.iteritems():
        for k in ('dstIPs','dstPorts','srcIPs','dstPorts'):
            if k not in f:
                f[k] = []

        a = classes.get(i) if i in classes else 0

        if f['dstIPs'] and f['dstPorts'] and f['srcIPs']:
            for d,dp,s in product(f['dstIPs'],f['dstPorts'],f['srcIPs']):
                for r in id3[(id3.src==ip2int(s))&(id3.dst==ip2int(d))&(id3.dport==dp)]['flow']:
                    y[scalar(r)] = a
        elif f['dstIPs'] and f['dstPorts']:
            for d,dp in product(f['dstIPs'],f['dstPorts']):
                for r in id3[(id3.dst==ip2int(d))&(id3.dport==dp)]['flow']:
                    y[scalar(r)] = a
        elif f['dstIPs']:
            for d in f['dstIPs']:
                for r in id3[(id3.dst==ip2int(d))]['flow']:
                    y[scalar(r)] = a
        elif f['srcIPs']:
            for d in f['srcIPs']:
                for r in id3[(id3.src==ip2int(d))]['flow']:
                    y[scalar(r)] = a

    return y,labeling,dict((i,annot) for i,(type,annot,f) in annotations.iteritems())
def show_spectra(opt,h5=None):
    #TODO -> model.py
    from models import pipeline,freq_bands,freq_treshold
    if not h5:
        h5 = File(opt.database,'a')
    if 'samples' not in h5:
        raise Exception('no samples found')
    samples = h5['samples']
    for k in samples.keys():
        if k.startswith('data'):
            if  '.srate' not in samples[k] or '.wndsize' not in samples[k]:
                continue
            if samples[k]['.srate'].value !=  opt.srate  or  samples[k]['.wndsize'].value != opt.window:
                continue
            X = samples[k]['X'][:]
            y = samples[k]['y'][:].squeeze()
            freqs = samples[k]['freqs'][:].squeeze()
            id = samples[k]['flowids']
            labeling,labeling2,labeling3 =  get_annotations(opt,id)
            lab  = np.array([labeling.get(f) for f in y.squeeze()])
            fnc = pipeline(freq_treshold(0), freq_bands(50))
            for i in  labeling2:
                if not (lab==i).any():
                    continue
                Xr,yr,freqr = fnc.fit(X[(lab==i) ,...],lab[(lab==i)],freqs)
                freqr = freqr.squeeze()
                fig(lambda ax: ax.bar(left=freqr[:-1],width=freqr[1:]-freqr[:-1],height=Xr.mean(0)),name=labeling2[i])
def show_mahal(opt,h5=None):
    #TODO -> model.py
    from models import pipeline,freq_bands,freq_treshold
    if not h5:
        h5 = File(opt.database,'a')
    if 'samples' not in h5:
        raise Exception('no samples found')
    samples = h5['samples']
    for k in samples.keys():
        if k.startswith('data'):
            if  '.srate' not in samples[k] or '.wndsize' not in samples[k]:
                continue
            if samples[k]['.srate'].value !=  opt.srate  or  samples[k]['.wndsize'].value != opt.window:
                continue
            X = samples[k]['X'][:]
            y = samples[k]['y'][:].squeeze()
            freqs = samples[k]['freqs'][:].squeeze()
            id = samples[k]['flowids']
            labeling,labeling2,labeling3 =  get_annotations(opt,id)
            lab  = np.array([labeling.get(f) for f in y.squeeze()])
            fnc = pipeline(freq_treshold(0), freq_bands(50))
            for i in  labeling2:
                if not (lab==i).any():
                    continue
                Xr,yr,freqr = fnc.fit(X[(lab==i) ,...],lab[(lab==i)],freqs)
                freqr = freqr.squeeze()
                fig(lambda ax: ax.bar(left=freqr[:-1],width=freqr[1:]-freqr[:-1],height=Xr.mean(0)),name=labeling2[i])
def get_raw(opt, callback=None, keys=(), h5=None):
    from extractor import PcapExtractor,FlowExtractor
    from os.path import isfile,basename
    from util import get_packets,get_netflow

    if opt.in_format == 'pcap':
        extract = PcapExtractor( ('time','src','sport','dst','dport','proto','paylen','flags', 'flow') )
        tr = h5['traces'] if h5 else None
        praser = get_packets
    elif opt.in_format == 'netflow':
        extract = FlowExtractor( ('time', 'duration','src','sport','dst','dport','proto', 'packets', 'size','flags', 'flows', 'flow') )
        tr = h5['netflows']  if h5 else None
        praser = get_netflow
        #def praser(fn, extract):
        #    f = open(fn,'r')
        #    try: flows = f.readlines()
        #    finally: f.close()
        #    return filter(None,map(extract,flows))
    else:
        raise NotImplementedError('in_format')

    if tr:
        if not callable(callback):
            callback = lambda data,fn: data.save(tr[fn])
        if not keys:
            keys = tr.keys()
    else:
        if not callable(callback):
            raise Exception('h5 file needed')

    for fn in opt.file:
        if isfile(fn) and basename(fn) not in keys:
            print '## Extracting features from file %s...' % fn
            pkts = praser(fn,extract)
            print '\t%d records captured'%len(pkts)
            data = Dataset(pkts, extract.fields)
            del pkts
            print '## Storing matrix in %s...' % opt.database
            callback(data,basename(fn))

def get_flow(opt, h5 = None):
    from util import timedrun
    from flowizer import Flowizer
    from operator import add

    result = [[],()]
    def process(data,flowize,postfix=''):
        print '## Extracting flows using %s-tuple'%opt.flowid
        fl = h5[flowid]
        q,f = flowize(data, fl['flowid']) if 'flowid' in fl else flowize(data)
        dataname= 'data_%s'%postfix if postfix else 'data'
        for i in (dataname,'flowids'):
            if i in fl:
                del fl[i]
        print '## Storing matrices in %s...' % opt.database
        f.save(fl[dataname])
        q.save(fl['flowids'])
        result[0].append(f)
        result[1] = q

    if not h5 :
        h5 = File(opt.database,'a')

    if opt.flowid == '3':
        fflow=('src','dst','dport')
        bflow=('dst','src','sport')
        flowid = 'flows3'
    elif opt.flowid == '4':
        fflow=('src', 'sport','dst','dport')
        bflow=('dst', 'dport','src','sport')
        flowid = 'flows4'
    else:
        raise NotImplementedError('flowid')

    if opt.in_format == 'pcap':
        fields = ('time', 'paylen', 'flow')
        tr = h5['traces'] if 'traces' in h5 else None
    elif opt.in_format == 'netflow':
        fields = ('time', 'size', 'packets', 'flow')
        tr = h5['netflows'] if 'netflows' in h5 else None
    else:
        raise NotImplementedError('in_format')

    flowize = timedrun(Flowizer(fields = fields,fflow=fflow,bflow=bflow,opt=opt))

    if not tr:
        fl = h5[flowid]
        keys = [ i[1] for i in (k.split('_',1) for k in fl.keys()) if len(i) > 1 ]
        get_raw(opt,lambda x,fn:process(x,flowize,fn),keys=keys)
    else:
        try:
            data = reduce(add,(tr[k] for k in sorted(tr.keys()))  )
            process(data,flowize)
        except MemoryError:
            for k in sorted(tr.keys()):
                data = tr[k]
                process(data,flowize,k)
    return tuple(result)

def get_samples(opt, h5= None):
    #TODO -> sampler.oy
    from sampler import csd,psd
    from dataset import Dataset,H5File
    from scipy.fftpack import fftfreq
    from sys import stdout
    from itertools import product
    from operator import add
    from util import flow2str,scalar

    if opt.flowid == '3':
        flowid = 'flows3'
    elif opt.flowid == '4':
        flowid = 'flows4'
    else:
        raise NotImplementedError('flowid')

    if opt.transform == 'psd':
        xsdfnc = psd
    elif opt.transform == 'csd':
        xsdfnc = csd
    else:
        raise NotImplementedError('transform')

    if opt.srate:
        srates = opt.srate
    else:
        raise ValueError('No sample rate specified.')

    if opt.window:
        windows = opt.window
    else:
        raise ValueError('No window length specified.')

    if not h5:
        h5 = H5File(opt)

    if flowid in h5:
        fl3 = h5[flowid]
        if 'data' in fl3:
            flows3 = fl3['data']
        else:
            keys = [k for k in fl3.keys() if k.startswith('data_')]
            if keys:
                flows3 = reduce(add,(fl3[k] for k in keys) )
            else:
                print '## no flow data in h5 database, trying to get it from raw files'
                if not opt.file:
                    raise Exception('No flow data specified. Try to specify some pcap or netflow files on command line.')
                flows3,id3 = get_flow(opt,h5 = h5)
                flows3 = reduce(add, flows3 )
        id3 = fl3['flowids']
    else:
        flows3,id3 = get_flow(opt,h5 = h5)
        flows3 = reduce(add, flows3 )

    samples = h5['samples']

    for srate,wndsize in product(srates,windows) : #srates:
        if srate<=0 :
            continue


        speriod = 1./ srate # sampling period in seconds
        wndspan = int(1e6 * wndsize * speriod) # window span in microseconds

        sampl = samples['data_%s_%f_%d'%(xsdfnc.__name__,srate,wndsize)]

        if '.srate' in sampl or '.wndsize' in sampl :
            if scalar(sampl['.srate']) != srate or scalar(sampl['.wndsize']) != wndsize:
                raise Exception('already processed for different srate (%s) and wndsize (%s)' %(scalar(sampl['.srate']),scalar(sampl['.wndsize'])))
            print '## already processed for same srate and wndsize - overwriting '
            for k in sampl.keys():
                del sampl[k]


        spectrums = {}
        amplitudes = {}
        wids = {}
        ips = {}

        # some colorful sugar
        ipfmt = lambda i: flow2str(i,dns=opt.reverse_dns,services=opt.reverse_dns, color=False)
        ipfmtc = lambda i: flow2str(i,dns=opt.reverse_dns,services=opt.reverse_dns, color=True)

        stdout.write('\n')
        stdout.flush()

        flows = {}
        for f in flows3['flow']:
            f = scalar(f)
            if f not in flows:
                flows[f] = 0
            else:
                flows[f] += 1
        flows = dict((k,v) for k,v in flows.items() if v>100)

        l = 1
        #for f in flid[:chooseflows,0]:
        for id3row in id3:
            f = scalar(id3row['flow'])
            if f not in flows:
                continue

            ips[f] = ipfmt(id3row)

            stdout.write('\rprogress: \033[33;1m%0.2f %%\033[0m, srate= \033[33;1m%f\033[0m Hz, wndsize= \033[33;1m%d\033[0m, \033[36mprocessing flow\033[0m: %s  '%
                         (100.*l/len(flows),srate,wndsize,ipfmtc(id3row)))
            stdout.flush()
            l += 1

            # select related packets
            #fl =  flows[(flows[...,2] == f ),...]
            fl =  flows3[flows3.flow==f]
            tm = fl['time']
            mi = tm.min()
            ma = tm.max()

            k = mi
            i = 0
            spectrum = []
            amplitude = []
            wid = []
            unused = 0

            while k<ma:
                # 10 dots progressbar
                if (ma-mi)>=(10*wndspan) and  not ((k-mi)/wndspan) % (((ma-mi)/(10*wndspan))):
                    stdout.write('\033[36m.\033[0m')
                    stdout.flush()

                #w = fl.data[(tm>=k) & (tm<k+wndsize*srate),...]
                if 'paylen' in fl:
                    w = fl[(fl.time>=k)&(fl.time<k+wndspan),'time','paylen']
                else:
                    w = fl[(fl.time>=k)&(fl.time<k+wndspan),'time','packets','size']

                if not len(w)>0:
                    unused += np.sum(w['packets']) if 'packets' in w else len(w)
                    k += wndspan
                    i += 1
                    continue

                # sampling intervals
                bounds = np.linspace(k, k+wndspan, wndsize, endpoint=True)[...,np.newaxis]

                amp,xsd = xsdfnc(w,bounds)

                if not xsd.any():
                    unused += np.sum(w['packets']) if 'packets' in w else len(w)
                    k += wndspan
                    i += 1
                    continue

                wid.append(i)
                spectrum.append(xsd)
                amplitude.append(amp)

                k += wndspan
                i += 1

            pkts = np.sum(fl['packets']) if 'packets' in fl else len(fl)

            if  len(amplitude):
                amplitude = np.vstack(a[np.newaxis,...] for a in amplitude)
                spectrum = np.vstack(a[np.newaxis,...] for a in spectrum)
                #amplitudes[f] = amplitude
                spectrums[f] = spectrum
                wids[f] = np.array(wid)
                if unused:
                    stdout.write('\r%s: unused \033[1;31m%d\033[0m of \033[1;34m%d\033[0m packets\033[K\n' %(ipfmtc(id3row),unused,pkts))
                else:
                    stdout.write('\r%s: used \033[1;34m%d\033[0m packets\033[K\n' %(ipfmtc(id3row),pkts))
            else:
                stdout.write('\r%s: unused \033[1;31m%d\033[0m packets\033[K\n' %(ipfmtc(id3row),pkts))
            stdout.write('\rprogress: \033[33;1m%0.2f %%\033[0m, srate= \033[33;1m%f\033[0m Hz, wndsize= \033[33;1m%d\033[0m   '%(100.*l/len(flows),srate,wndsize))
            stdout.flush()

        stdout.write('\rprogress: \033[33;1m100 %%\033[0m, srate= \033[33;1m%f\033[0m Hz, wndsize= \033[33;1m%d\033[0m           \n'% (srate,wndsize))
        stdout.flush()

        if len(spectrums):
            flows = list(spectrums.keys())

            X = np.vstack(spectrums[f] for f in flows) # spectrums
            #ampl = np.vstack(amplitudes[f] for f in flows) # amplitudes
            y = np.vstack(np.array([[f]]).repeat(spectrums[f].shape[0],0) for f in flows) # flows
            #wnds = np.vstack(wids[f][...,np.newaxis] for f in flows) # windows kept

            sampl['.srate'] = srate
            sampl['.wndsize'] = wndsize

            sampl['X'] = X
            sampl['y'] = y
            sampl['freqs'] = fftfreq(wndsize-1,d=1./srate)

            id3.save(sampl['flowids'])


def annotate(opt, h5=None):
    """ Crappy feature allowing to create annotation file
    """
    from util import flow2str,scalar
    from operator import add
    from sys import stdout
    from dataset import H5File
    import json

    def feedback(i):
        types = {'2':'FILTER_MALICIOUS','1':'FILTER_LEGITIMATE', '0': None}
        input = ()
        while len(input)!=2 or input[0].strip() not in types:
            input = raw_input('%s ? '%flow2str(i,dns=True,services=True)).split(',',1)
        return {'type':types.get(input[0].strip()),'annotation':input[1],'data':i}

    if not h5 :
        h5 = H5File(opt)

    if opt.flowid == '3':
        flowid = 'flows3'
    elif opt.flowid == '4':
        flowid = 'flows4'
    else:
        raise NotImplementedError('flowid')

    d  = h5['%s/flowids'%flowid]
    data = reduce(add,  (h5['%s/%s'%(flowid,k)] for k in h5['flows3'].keys() if k.startswith('data')))
    flows = [i for i in reversed(sorted(((i,len(data[data.flow==i['flow']])) for i in d),key=lambda x: x[-1]))]
    min_packets =  opt.min_packets if opt.min_packets else 100
    flows = [i for i,c in flows if c>min_packets]
    print '## %d records to annotate'%len(flows)
    filt = [feedback(i) for i in sorted(flows,key=lambda x: x['dport'])]
    filt2 = [
        {
            'dstIPs':list(set( int2ip(scalar(j['data']['dst'])) for j in filt if j['annotation'] == a)),
            'dstPorts':list(set( scalar(j['data']['dport']) for j in filt if j['annotation'] == a)),
            'annotation':a,
            'type':t
        } for a,t in set((i['annotation'],i['type']) for i in filt)
    ]
    try:
        out = open(opt.out_file,'w') if  opt.out_file else stdout
    except :
        out =  stdout
    json.dump(filt2,out)

def opts(args=None):
    """ User interface features
    """
    from argparse import ArgumentParser
    from sys import argv
    from info import __description__,__version__,__prog__,__date__,__author__

    if not args: args = argv[1:]

    actions = ('raw','flow','sample','model','filter', 'load', 'annotate')
    flowids  = ('3','4')
    transforms  = ('csd','psd')
    filetypes  = ('pcap','netflow')

    parser = ArgumentParser(description=__description__)


    parser.add_argument('--version', action='version', version='%s v%s\nCopyright (C) %s %s'%(__prog__,__version__,__date__.split('-',1)[0],__author__), help= 'show version information')

    parser.add_argument('action',choices=actions, metavar='(%s)'%('|'.join(actions)), help= ''\
            'action to execute; raw stores "pcap" or netflow data in h5 database, "flow" marks flows and extracts attributes, '\
            '"sample" computes sampling at given sample rate and tranformations at given windowing, "model" fits '\
            'model to data stored in database, filter converts XML Ip filters to JSON format and "load" loads'\
            ' database into memory')
    parser.add_argument('database', metavar='<database file>', help='hdf5 array database')
    parser.add_argument('file', nargs='*', metavar='<input file>', help='input files to process')

    parser.add_argument('-f', dest='in_format',choices=filetypes, metavar='(%s)'%('|'.join(filetypes)), help='input file format')
    parser.add_argument('-o', dest='out_file', metavar='<output file>', help='output file')
    parser.add_argument('-m', dest='min_packets', metavar='<min packets>', type=int, help='min packets per flow')
    parser.add_argument('-n', dest='reverse_dns', action='store_false', help='don`t do reverse dns')

    group = parser.add_mutually_exclusive_group()
    group.add_argument('-v', '--verbose', dest='verbosity', action='count', help='increase verbosity')
    group.add_argument('-q', '--quiet', dest='verbose', action='store_false', help='do not dump to terminal')

    group= parser.add_argument_group('Flow extraction options', 'Required for "flow", "sample" and "model" actions')
    group.add_argument('-i', dest='flowid',choices=flowids, metavar='(%s)'%('|'.join(flowids)), help='flow identification (3-tuple or 4-tuple)')
    group.add_argument('-u', dest='usesyns', action='store_true', help='don`t use SYN packets to distinguish flow start')
    group.add_argument('-p', dest='protocol', type=int, help='protocol to take in account, default = 6 (TCP)')

    group = parser.add_argument_group('Sampling options', 'Required for "sample" and "model" actions')
    group.add_argument('-s', dest='srate', metavar='<sample rate>',action='append',type=float,help='sample rate to use, can be specified multiple times')
    group.add_argument('-w', dest='window', metavar='<window length>',action='append',type=int,help='window lengths to use, can be specified multiple times')
    group.add_argument('-t', dest='transform', metavar='(%s)'%('|'.join(transforms)), choices=transforms,help=''\
          'tranformation to use, can be: "csd" for cross spectral density or "psd" for power spectral density')

    group = parser.add_argument_group('Model estimation options', 'Required for "model" action')
    group.add_argument('-a', dest='annotations', metavar='<file>', help='annotation file')
    #group.add_argument()

    parser.set_defaults(verbose=True,verbosity=0,reverse_dns=True,usesyns=True,protocol=6)

    #parser.print_help()
    longopts =  dict((v.dest,k) for k,v in  parser._option_string_actions.iteritems() if k.startswith('--'))
    shortopts =  dict((v.dest,k) for k,v in  parser._option_string_actions.iteritems() if not k.startswith('--'))
    opt =  parser.parse_args(args)

    if opt.action in ('raw','flow') and not opt.in_format:
        parser.error('input file format (--input-format) not specified for "raw" or "flow" action')

    if opt.action in ('flow', 'sample') and not opt.flowid:
        parser.error('flow identification (--flowid) not specified for "flow" or "sample" action')

    if opt.action in ('sample') and not opt.srate:
        parser.error('sample rate (--srate) not specified for "sample" action')

    if opt.action in ('sample') and not opt.window:
        parser.error('window (--window) not specified for "sample" action')

    if opt.action in ('sample') and not opt.transform:
        parser.error('transform (--transform) not specified for "sample" action')

    return opt, longopts, shortopts



if __name__=='__main__':
    from util import ip2int,int2ip,reverseDns,fig,scalar
    from dataset import Dataset,H5File
    from info import __graphics__
    from sys import stdout

    import numpy as np
    import util
    import models
    import logging

    logging.captureWarnings(True)
    logging.basicConfig(level='ERROR')

    stdout.write(__graphics__)

    opt, longopts, shortopts = opts()
    NotImplementedError = lambda s: ValueError('Choice %s %s not implemented'%(longopts.get(s) or shortopts.get(s),getattr(opt,s)))


    if opt.action == 'filters':

        import json
        from os.path import isfile
        from util import get_filter
        from sys import stdout
        f = open(opt.out_file,'w') if opt.out_file else stdout
        try:
            print json.dump([get_filter(f.strip()) for f in opt.file if isfile(f)], f)
        finally:
            f.close()

    elif opt.action == 'load':

        l = H5File(opt)

    elif opt.action == 'raw':

        H5File(opt).handle_exit(get_raw)

    elif opt.action == 'annotate':

        H5File(opt).handle_exit(annotate)

    elif opt.action == 'flow':

        H5File(opt).handle_exit(get_flow)

    elif opt.action == 'sample':

        H5File(opt).handle_exit(get_samples)

    elif opt.action == 'model':
        #TODO -> model.py
        from models import freq_treshold,freq_bands,freq_momentum,freq_sdev,freq_low_hi,pca,scale,gmm,pipeline,FitError,mahal
        h5 = H5File(opt)


        def iterate(name, *args):
            from itertools import product
            cmds = [arg[0] for arg  in args]
            params = product(*[arg[1] for arg  in args])
            return [ (tuple((cmds[i].__name__,param[i]) for i in range(len(cmds))),pipeline(*[cmds[i](param[i]) for i in range(len(cmds))])) for param in params ]


        def crossval(method, X, y, freqs, folds = 10):
            from sklearn.cross_validation import StratifiedKFold
            s = []
            d = []
            for train, test in StratifiedKFold(y.squeeze(), folds, indices=False):
                try:
                    method.fit(X[train,...],y[...,train],freqs)
                    (score,data),dummy,dummy = method.score(X[test,...], y[...,test], freqs)
                    s += score,
                    d += data,
                except FitError:
                    s+=-np.inf,
            return np.array(s),d
        try:
            methods = {}
            #methods.update(iterate( 'bands', ( freq_treshold, (0,)) , ( freq_bands, (2,5,10,25))  , ( scale, (True,)) ,  ( gmm, (1,) ) ))
            methods.update(iterate( 'bands', ( freq_treshold, (0,)) , ( freq_bands, (5,10))   ,  ( mahal, (0,) ) ))
            methods.update(iterate( 'bands', ( freq_treshold, (0,)) ,  ( mahal, (0,) ) ))
            methods.update(iterate( 'bands2', ( freq_treshold, (0,)) , ( freq_bands, (2,10))  ,  ( gmm, (5,) ) ))
            #methods.update(iterate( 'sdev', ( freq_treshold, (0,)) , ( freq_sdev, (0,)) , ( scale, (True,)) ,  ( gmm, (1,) ) ))
            methods.update(iterate( 'momentum', ( freq_treshold, (0,)) , ( freq_momentum, (14,)) , ( scale, (True,)) ,  ( gmm, (1,) ) ))
            methods.update(iterate( 'momentum', ( freq_treshold, (0,)) , ( freq_momentum, (14,))  ,  ( mahal, (1,) ) ))
            #methods.update(iterate( 'lowhi', ( freq_treshold, (0,)) ,( freq_low_hi, (5,10,20,40)) , ( scale, (True,)) ,  ( gmm, (1,) ) ))
            methods.update(iterate( 'pca', ( freq_treshold, (0,)) ,  ( pca, (5,)) , ( scale, (True,)) ,  ( gmm, (1,) ) ))
            #methods.update(iterate( 'lowhi', ( freq_treshold, (0,)) ,( freq_low_hi, (5e-6,6e-5,1.1e-4,1.6e-4,2.2e-4, 2.7e-4, 3.2e-4, 3.8e-4, 4.4e-4, 5e-5 )) , ( scale, (True,)) ,  ( gmm, (1,) ) ))
            samples = h5['samples']

            print '\033[34;1mdatasets found in database %s:\033[0m' %opt.database
            datasets = []
            i = 0
            for k in samples.keys():
                if not k.startswith('data'):
                    continue
                sampl = samples[k]

                if  '.srate' not in sampl or  '.wndsize' not in sampl :
                    continue

                srate = scalar(sampl['.srate'])
                wndsize = scalar(sampl['.wndsize'])

                if srate not in opt.srate or wndsize not in opt.window:
                    continue

                print '[\033[33;1m%d\033[0m] \033[32m%s\033[0m : (srate=%f, wndsize=%d)'%(i,k,srate,wndsize)

                datasets.append((i,(k,sampl,srate,wndsize)))
                i+=1
            datasets = dict(datasets)

            if len(datasets)>1:
                selected = []
                while not selected:
                    s = raw_input('datasets to use:')
                    selected = [datasets[int(i.strip())] for i in s.split(',')]
            else:
                selected = datasets.values()

            for k,sampl,srate,wndsize in selected:
            #for k in samples.keys():
                try:
                    #if not k.startswith('data'):
                    #    continue
                    #sampl = samples[k]

                    #srate = sampl['.srate'].value
                    #wndsize = sampl['.wndsize'].value

                    #if srate not in opt.srate or wndsize not in opt.window:
                    #    continue

                    print '## processing %s'%k

                    X = sampl['X'][:]
                    y = sampl['y'][:]
                    freqs = sampl['freqs'][:]

                    id3 = sampl['flowids']

                    labeling,labeling2,labeling3 = get_labeling(opt,id3)

                    #labels = np.array([[labeling.get(f) for f in y.squeeze()]]) # annotations

                    y =  np.array([[labeling.get(f) if f in labeling else 0 for f in y.squeeze()]])
                    print '## please hold on'
                    #X, y, freqs, srate, wndsize = get_sampl(k)

                    f = []
                    n = []
                    scores = []
                    results = []
                    for p,meth in methods.items():
                        s,d = crossval(meth, X, y, freqs)
                        results.append('\t%s: score (mean=%f, std=%f)' % (','.join('%s(%d)'%n for n in p), s.mean(), s.std()))
                        f += d[0],
                        n += 'srate=%f, %s' % (srate,'%s(%d)' % p[1]),
                        scores += s,
                    print 'srate=%fHz, wndsize=%dsamps'%(srate,wndsize)
                    print '\n'.join(results)
                    def plotline(x,y):
                        return lambda ax: ax.plot(x, y, '-')
                    fig(list(plotline(fpr, tpr) for fpr, tpr, t in f),name=n,show=True)
                except Exception as e:
                    print e
                    raise
        finally:
            pass
            #h5.close()
    elif opt.action == 'feature':
        #TODO -> model.py
        from dataset import Variable,Dataset
        from scipy.fftpack import fftfreq,fft
        from sys import stdout
        from util import scatter,scalar

        flow = Variable('flow')
        src = Variable('src')
        dst = Variable('dst')
        dport = Variable('dport')
        sport = Variable('sport')

        h5 = File(opt.database,'a')

        if len(argv)>3:
        #for grp in (k for k in h5.keys() if k.startswith('samples_')):
        #for i in (100,200,500,1000,2000):
            grp = argv[3]
            #if grp not in h5:
            #    exit() # continue

            sampl = h5[grp]

            srate = sampl['.srate'].value
            wndsize = sampl['.wndsize'].value

            X = sampl['X'].value
            y = sampl['y'].value
            freqs = sampl['freqs'].value

            id3 = Dataset(data=sampl['id'].value,fields=sampl['idfields'].value)
            labeling,labeling2 = get_labeling(id3)
            labels = np.array([[labeling.get(f) for f in y.squeeze()]]) # annotations

            def pcatransform(ndim=2):
                from sklearn.decomposition import PCA
                t = PCA(ndim)
                def fnc(X):
                    t.fit(X[labels.squeeze()==1,...])
                    return t.transform(X)
                return fnc
            def lowhitransform(srate, wndsize, fthresh=50):
                from scipy.fftpack import fftfreq
                freqs = fftfreq(wndsize-1,d=1./srate)
                return lambda X: np.vstack((X[...,freqs<=fthresh].sum(1),X[...,freqs>fthresh].sum(1))).transpose()
            def varnorm(X):
                return (X - X.mean(0).reshape((1,X.shape[1])) )/X.std(0).reshape((1,X.shape[1]))
            def logistnorm(X):
                from scipy.special import expit
                return expit(varnorm(X))
            def scatter1(ax):
                return scatter(ax, X, labels, varnorm, pcatransform(ndim=2), labeling2.get)
            def scatter1a(ax):
                return scatter(ax, X, labels, varnorm, pcatransform(ndim=2), labeling2.get)
            def scatter1b(ax):
                return scatter(ax, X, labels, logistnorm, pcatransform(ndim=2), labeling2.get)
            def scatter2a(ax):
                return scatter(ax, X, labels, None, lowhitransform(srate,wndsize,fthresh=10), labeling2.get)
            def scatter2b(ax):
                return scatter(ax, X, labels, varnorm, lowhitransform(srate,wndsize,fthresh=10), labeling2.get)
                #fignames =  'spectral features (srate=%dHz, wnd=%ds), %%s, %%s' %(srate,wndsize/srate)
            #fignames = [fignames%('variance normalization','2D pca projection'),fignames%('logistic normalization','2D pca projection')]
            #fig([scatter1a,scatter1b], fignames,show=True)
            #fignames =  'spectral features (srate=%dHz, wnd=%fs), %%s, %%s' %(srate,1.*wndsize/srate)
            #fignames = [fignames%('no normalization','low-pass/hi-pass energy'),fignames%('variance normalization','low-pass/hi-pass energy')]
            fig(scatter1,show=True)
